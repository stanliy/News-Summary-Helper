import feedparser
import time, datetime
import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../")))
from utils.connection import updateDB
from newspaper import Article
from sqlalchemy import text

# ğŸ“Œ RSS í”¼ë“œ ì„¤ì •
RSS_FEED = [
    ("https://www.khan.co.kr/rss/rssdata/economy_news.xml", "ê²½ì œ"),
    ("https://www.khan.co.kr/rss/rssdata/politic_news.xml", "ì •ì¹˜"),
    ("https://www.khan.co.kr/rss/rssdata/society_news.xml", "ì‚¬íšŒ"),
    ("https://www.khan.co.kr/rss/rssdata/kh_world.xml", "êµ­ì œ"),
    ("https://www.khan.co.kr/rss/rssdata/culture_news.xml", "ë¬¸í™”"),
    ("https://www.khan.co.kr/rss/rssdata/kh_sports.xml", "ìŠ¤í¬ì¸ "),
    ("https://www.khan.co.kr/rss/rssdata/science_news.xml", "ê³¼í•™"),
]

for rss_feed in RSS_FEED:
    # 1ï¸âƒ£ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ URL ê°€ì ¸ì˜¤ê¸°
    feed = feedparser.parse(rss_feed[0])
    print("RSS URL:", rss_feed[0])

    print("Feed keys:", feed.keys())  # feed.entries ì™¸ì— ì–´ë–¤ í‚¤ê°€ ìˆëŠ”ì§€ ë³´ê¸°
    print("Entries ê°œìˆ˜:", len(feed.entries))

    print(f"RSSì—ì„œ {len(feed.entries)}ê°œì˜ ê¸°ì‚¬ ë°œê²¬!")

    # 2ï¸âƒ£ ê° ê¸°ì‚¬ í¬ë¡¤ë§
    sqlList = []
    paramList = []
    crawl_count = 10
    if len(feed.entries) < 10:
        crawl_count = len(feed.entries)
    for ii in range(0, crawl_count):
        entry = feed.entries[ii]
        url = entry.link
        print(f"\n[ê¸°ì‚¬ URL] {url}")

        try:
            # newspaper3k ì‚¬ìš©
            article = Article(url, language='ko')
            article.download()
            article.parse()


            print(f"ì œëª©: {article.title}")
            print(f"ë¶„ì•¼: {rss_feed[1]}")
            print(f"ë‚ ì§œ: {str(article.publish_date)[:10]}")
            print(f"ë³¸ë¬¸ (10ì): {article.text[:10]}...")

            # âœ… ì—¬ê¸°ì„œ DB ì €ì¥ or íŒŒì¼ ì €ì¥ ê°€ëŠ¥ (ì˜ˆ: CSV, SQLite ë“±)
            sql = text("""
            INSERT INTO related_news (url, category, title, date, content)
            VALUES (:url, :category, :title, :date, :content)
            """)

            params = {
                "url": entry.link,
                "category": rss_feed[1],
                "title": article.title,
                "date": str(article.publish_date)[:10],
                "content": article.text
            }

            # ê¸ˆì¼ ê¸°ì‚¬ë§Œ í¬ë¡¤ë§í•˜ë„ë¡ í•„í„°ë§
            # if str(article.publish_date)[:10] == str(datetime.datetime.now())[:10]:
            #     print('this is today news')
            sqlList.append(sql)
            paramList.append(params)

        except Exception as e:
            print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

        time.sleep(1)  # polite crawling (1ì´ˆ ëŒ€ê¸°)
    result = updateDB(sqlList, paramList)
    if result['status'] == "success":
        print("done!")