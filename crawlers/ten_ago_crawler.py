from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from datetime import datetime
import pytz
import time
import json

CATEGORY_LIST = ["ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "êµ­ì œ", "ë¬¸í™”", "ìŠ¤í¬ì¸ ", "ê³¼í•™Â·í™˜ê²½"]


def dedup_preserve_order(seq):
    seen = set()
    return [x for x in seq if not (x in seen or seen.add(x))]


def collect_khan_news_10_years_ago(keyword="", page_count=1):
    kr_tz = pytz.timezone("Asia/Seoul")
    now_kr = datetime.now(kr_tz)
    try:
        target_date = now_kr.replace(year=now_kr.year - 10)
    except ValueError:
        target_date = now_kr.replace(month=2, day=28, year=now_kr.year - 10)

    target_str = target_date.strftime("%Y-%m-%d")
    urls = []

    service = Service('./chromedriver.exe')
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    driver = webdriver.Chrome(service=service, options=options)

    for page in range(1, page_count + 1):
        search_url = (
            f"https://search.khan.co.kr/?q={keyword}"
            f"&media=khan&section=0&term=5"
            f"&startDate={target_str}&endDate={target_str}"
            f"&page={page}&sort=0"
        )

        print(f"\nğŸ” [{keyword}] ê²€ìƒ‰ ë‚ ì§œ: {target_str}")
        print("ğŸŒ URL:", search_url)

        driver.get(search_url)
        time.sleep(2)

        articles = driver.find_elements(By.CSS_SELECTOR, "article a")
        print("ğŸ“° ê¸°ì‚¬ ìˆ˜:", len(articles))

        for article in articles:
            link = article.get_attribute('href')
            if link and link.startswith("https://www.khan.co.kr/article/"):
                print("âœ… ìˆ˜ì§‘ëœ ë§í¬:", link)
                urls.append(link)

    driver.quit()
    return dedup_preserve_order(urls)


def find_first_article_by_category(driver, urls, target_category):
    for idx, url in enumerate(urls, 1):
        print(f"\nğŸ” ({idx}/{len(urls)}) [{target_category}] ê¸°ì‚¬ í™•ì¸ ì¤‘: {url}")
        try:
            driver.get(url)
            time.sleep(1)

            try:
                tag = driver.find_element(By.CSS_SELECTOR, "a.category")
                category = tag.get_attribute("title") or tag.text
            except:
                tag = driver.find_element(By.CSS_SELECTOR, "ul.breadcrumb > li:nth-child(2)")
                category = tag.text.strip()

            category = ' '.join(category.strip().split())
            print(f"âœ… [{category}] {url}")

            if category == target_category:
                print(f"ğŸ¯ [{category}] ê¸°ì‚¬ ë°œê²¬! â†’ {url}")
                return url

        except Exception as e:
            print(f"âŒ í™•ì¸ ì‹¤íŒ¨: {url}\nâ›” {e}")
            continue

    return None


def find_articles_for_all_categories(page_count=1):
    results = {}

    CATEGORY_KEYWORD_MAP = {
        "ì •ì¹˜": "ì •ì¹˜",
        "ê²½ì œ": "ê²½ì œ",
        "ì‚¬íšŒ": "ì‚¬íšŒ",
        "êµ­ì œ": "êµ­ì œ",
        "ë¬¸í™”": "ë¬¸í™”",
        "ìŠ¤í¬ì¸ ": "ì¶•êµ¬",
        "ê³¼í•™Â·í™˜ê²½": "í™˜ê²½",  # âœ… ê²€ìƒ‰ í‚¤ì›Œë“œë§Œ ë°”ê¿ˆ
    }

    # âœ… ë‚ ì§œ ê³„ì‚° (10ë…„ ì „ ì˜¤ëŠ˜)
    kr_tz = pytz.timezone("Asia/Seoul")
    now_kr = datetime.now(kr_tz)
    try:
        target_date = now_kr.replace(year=now_kr.year - 10)
    except ValueError:
        target_date = now_kr.replace(month=2, day=28, year=now_kr.year - 10)
    target_str = target_date.strftime("%Y-%m-%d")

    # âœ… í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰
    service = Service('./chromedriver.exe')
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    driver = webdriver.Chrome(service=service, options=options)

    try:
        for category, keyword in CATEGORY_KEYWORD_MAP.items():
            urls = collect_khan_news_10_years_ago(keyword=keyword, page_count=page_count)
            url = find_first_article_by_category(driver, urls, target_category=category)
            results[category] = {
                "url": url,
                "date": target_str
            } if url else None
    finally:
        driver.quit()

    return results


if __name__ == "__main__":
    final_results = find_articles_for_all_categories(page_count=2)
    
    with open("final_results.json", "w", encoding="utf-8") as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)
    
    print("\nğŸ“Œ ë¶„ì•¼ë³„ ëŒ€í‘œ ê¸°ì‚¬:")
    for cat, url in final_results.items():
        print(f"{cat}: {url if url else 'âŒ ì—†ìŒ'}")